<document xmlns="http://cnx.rice.edu/cnxml" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:md="http://cnx.rice.edu/mdml">
  <title>Normalized LMS</title>
  <metadata><md:content-id>undefined</md:content-id><md:title/><md:uuid>e131fb4b-47e9-4290-a02a-763e9dd62075</md:uuid>
</metadata>

  <content>
    <para id="delete_me">
       In "normalized" LMS, the gradient step factor
       <m:math><m:ci>μ</m:ci></m:math> is normalized by the energy
       of the data vector:
      <m:math display="block">
	<m:apply>
	  <m:eq/>
	  <m:ci>
	    <m:msub>
	      <m:mi>μ</m:mi>
	      <m:mi>NLMS</m:mi>
	    </m:msub>
	  </m:ci>
	  <m:apply>
	    <m:divide/>
	    <m:ci>α</m:ci>
	    <m:apply>
	      <m:plus/>
	      <m:apply>
		<m:times/>
		<m:ci type="vector">
		  <m:msubsup>
		    <m:mi>X</m:mi>
		    <m:mi>k</m:mi>
		    <m:mi>H</m:mi>
		  </m:msubsup>
		</m:ci>
		<m:ci type="vector">
		  <m:msub>
		    <m:mi>X</m:mi>
		    <m:mi>k</m:mi>
		  </m:msub>
		</m:ci>
	      </m:apply>
	      <m:ci>σ</m:ci>
	    </m:apply>
	  </m:apply>
	</m:apply>
      </m:math>
      where <m:math><m:ci>α</m:ci></m:math> is usually 
      <m:math>
	<m:apply>
	  <m:divide/>
	  <m:cn>1</m:cn>
	  <m:cn>2</m:cn>
	</m:apply>
      </m:math>
      and <m:math><m:ci>σ</m:ci></m:math> is a very small number
    introduced to prevent division by zero if
      <m:math>
	<m:apply>
	  <m:times/>
	  <m:ci type="vector">
	    <m:msubsup>
	      <m:mi>X</m:mi>
	      <m:mi>k</m:mi>
	      <m:mi>H</m:mi>
	    </m:msubsup>
	  </m:ci>
	  <m:ci type="vector">
	    <m:msub>
	      <m:mi>X</m:mi>
	      <m:mi>k</m:mi>
		  </m:msub>
	  </m:ci>
	</m:apply>
      </m:math> is very small.
      <m:math display="block">
	<m:apply>
	  <m:eq/>
	  <m:ci type="vector">
	    <m:msub>
	      <m:mi>W</m:mi>
	      <m:mrow>
		<m:mi>k</m:mi>
		<m:mo>+</m:mo>
		<m:mn>1</m:mn>
	      </m:mrow>
	    </m:msub>
	  </m:ci>
	  <m:apply>
	    <m:plus/>
	    <m:ci>
	      <m:msub>
		<m:mi>W</m:mi>
		<m:mi>k</m:mi>
	      </m:msub>
	    </m:ci>
	    <m:apply>
	      <m:times/>
	      <m:apply>
		<m:divide/>
		<m:cn>1</m:cn>
		<m:apply>
		  <m:times/>
		  <m:ci type="vector">
		    <m:msup>
		      <m:mi>X</m:mi>
		      <m:mi>H</m:mi>
		    </m:msup>
		  </m:ci>
		  <m:ci type="vector">X</m:ci>
		</m:apply>
	      </m:apply>
	      <m:ci>
		<m:msub>
		  <m:mi>e</m:mi>
		  <m:mi>k</m:mi>
		</m:msub>
	      </m:ci>
	      <m:ci type="vector">
		<m:msub>
		  <m:mi>X</m:mi>
		  <m:mi>k</m:mi>
		</m:msub>
	      </m:ci>
	    </m:apply>
	  </m:apply>
	</m:apply>
      </m:math>
      The normalization has several interpretations
      <list id="list1" list-type="enumerated">
	<item>corresponds to the 2nd-order convergence bound</item>
	<item>makes the algorithm independent of signal scalings</item>
	<item>adjusts
	  <m:math>
	    <m:ci>
	      <m:msub>
		<m:mi>W</m:mi>
		<m:mrow>
		  <m:mi>k</m:mi>
		  <m:mo>+</m:mo>
		  <m:mn>1</m:mn>
		</m:mrow>
	      </m:msub>
	    </m:ci>
	  </m:math>
	  to give zero error with current input:
	  <m:math>
	    <m:apply>
	      <m:eq/>
	      <m:apply>
		<m:times/>
		<m:ci>
		  <m:msub>
		    <m:mi>W</m:mi>
		    <m:mrow>
		      <m:mi>k</m:mi>
		      <m:mo>+</m:mo>
		      <m:mn>1</m:mn>
		    </m:mrow>
		  </m:msub>
		</m:ci>
		<m:ci>
		  <m:msub>
		    <m:mi>X</m:mi>
		    <m:mi>k</m:mi>
		  </m:msub>
		</m:ci>
	      </m:apply>
	      <m:ci>
		<m:msub>
		  <m:mi>d</m:mi>
		  <m:mi>k</m:mi>
		</m:msub>
	      </m:ci>
	    </m:apply>
	  </m:math>
	</item>
	<item>minimizes mean effort at time
	  <m:math>
	    <m:apply>
	      <m:plus/>
	      <m:ci>k</m:ci>
	      <m:cn>1</m:cn>
	    </m:apply>
	  </m:math>
	</item>
      </list>
	NLMS usually converges <emphasis>much</emphasis> more quickly than LMS at very little
	extra cost; NLMS is <emphasis>very</emphasis> commonly used.
        In some applications, normalization is so universal that
        "we use the LMS algorithm" implies normalization as well.
  
    </para>

  </content>
  
</document>